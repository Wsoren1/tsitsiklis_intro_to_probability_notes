{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is Statistical Inference?\n",
    "\n",
    "Statistical inference is the process of extracting information of an unknown variable or an unknown model from available data. \n",
    "There are two main types, Bayesian and Classical.  \n",
    "\n",
    "Inference problems are normally categorized into Parameter Estimation, Hypothesis testing, and Significance Testing. The methodologies discussed in the textbook are Maximum a posteriori probability rule, least mean squares estimation, maximum likelihood, regression, and likelihood ratio tests."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Probability vs. Statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall probability is defined by axioms, so we must assume a probabilistic model follows the axioms.  We can then use methods to extract information, and have a set, unique correct answer.  This differs from Statistics, since in Stats, there are many different methods to extract different answers, and there is no best method.  Choosing a method is described as an art, and considerations depend on factors such as performance guarantees, past experience, \"common sense,\" and the consensus of the statistics community. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bayesian Statistics vs. Classical Statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Bayesian:__\n",
    "- Models are treated as random variables with known distributions.\n",
    "- Fits statistical methods into probability theory axioms, so each question has unique answer\n",
    "    - How is this done?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A bayesian model introduces a random variable $\\Theta$, that represents the model, and postulates a __prior__ probability distribution $p_\\Theta(\\theta)$. The following model, the __posterior__ probability distribution captures all the information that $x$ can provide about $\\theta$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>$p_{\\theta|x}(\\theta|x)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Classical:__\n",
    "- Models are treated as deterministic quantities, which happen to be unknown.\n",
    " - specifically, $\\theta$ in this case is seen as a constant unknown quantity, then is estimated with some performance guarantees\n",
    "- So, $\\theta$ could be any probabilistic model\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__So who is the winner?__\n",
    "\n",
    "Neither. It is a heavily debated topic, but each clearly are powerful schools.\n",
    "\n",
    "The debate in the textbook is presented as such:\n",
    "\n",
    "Consider trying to determine the mass of the electron, through noisy experiments.  Classical statisticians argue that it is a constant, and is unreasonable to model it as a random variable.  Bayesian statisticians will counter that a prior distribution would reflect our state of knowledge, as in, if we know from past experiments that there is a reasonable range for the mass of an electron, we can express this by postulating a prior distribution which is concentrated within that range.\n",
    "\n",
    "Classical statisticians will argue that it is arbitrary when picking a prior distribution, however, each classical statistical procedure will contain some hidden choices.  Bayesian statisticians would argue that since this is more clearly defined, they make the choice available for debate.\n",
    "\n",
    "Bayesian methods can have difficulty when computing, such as the evaluation of multi-dimensional integrals, that are now possible and accessible due to advances in computing power."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Inference vs. Variable Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Model Inference:__ The object of study is a real process where we want to validate a model based on available data\n",
    "- Ex: do planets follow elliptical Trajectories?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Variable Inference:__ What is the value of an unknown variable in our data?\n",
    "- Ex: What is my position, given a few GPS readings?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In some cases, we can build a model and use it to figure out variables, or find out variables to build a model.  Each type of inference can be used to construct the other information.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rough Classification of Statistical Inference Problems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Estimation:__ A model is fully specified, except for an unknown parameter $\\theta$ which we want to estimate.  It could be either a random variable or an unknown constant. The overarching goal: estimate $\\theta$ that is close to the true value.\n",
    "\n",
    "Examples:\n",
    "\n",
    "- Using polling data, estimate the fraction of the voter population that prefers candidate A over candidate B.\n",
    "\n",
    "- Given historical stock market data, estimate the mean and variance of the daily movement in the price of a particular stock"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Binary hypothesis testing:__ We start with two hypotheses, and use available data to decide which of the two is true.\n",
    "\n",
    "Examples:\n",
    "- Given a noisy picture, decide if there is a person in the picture or not.\n",
    "- Given a set of trials with two alternative medical treatments, decide which treatment is most effective.\n",
    "\n",
    "There is also *m*__-ary hypothesis testing__ problems, where there are _m_ competing hypotheses.  \n",
    "\n",
    "In estimation and hypothesis testing, bayesian or classical methods can be used"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This chapter will cover mainly Bayesian estimation, and a bit of hypothesis testing.  Next chapter will discuss broader ranges of these problems.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Major Terms, Problems, and Methods in this Chapter__\n",
    "- __Bayesian statistics__ treats unknown parameters as random variables with known prior distributions\n",
    "- In __parameter estimation__, we want to generate estimates that are close to the true values of the parameters in some probabilistic sense\n",
    "- In __hypothesis testing__, the unknown parameter takes one of a finite number of values, corresponding to competing hypothesis; we want to choose one of the hypotheses, aiming to acheive a small probability of error.\n",
    "- Principle Bayesian inference methods:\n",
    " - __Maximum a posteriori probability__ (MAP) rule: Out of the possible parameter values/hypotheses, select one with the maximum conditional/posterior probability given the data (section 8.2)\n",
    " - __Least mean squares__ (LMS) estimation: Select an estimator/function of the data that minimizes the mean squared error between the parameter and its estimate (section 8.3)\n",
    " - __Linear LMS__ estimation: Select an estimator that is a linear function of the data and minimizes the mean squared error between the parameter and its estimate (section 8.4). This may result in larger LMS, but requires simple calculations, based only on the means, variances, and covariances of the random variables involved."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8.1 Bayesian Inference and the Posterior Distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To conduct bayesian inference, we take our unknown quantity, $\\Theta$ as a random variable, or a finite collection of random variables.  Our goal is to take $X$, which is a collection of known related random variables $(X_1, ..., X_n)$ which we call __Observations, Measurements,__ or an __Observation vector__.  We then assume that we know the joint distribution of $\\Theta$ and $X$.  This translates to the following:\n",
    "- A Prior distribution $p_\\Theta$ or $f_\\Theta$, depending on whether $\\Theta$ is discrete or continuous\n",
    "- A Conditional distribution $p_{X|\\Theta}$ or $f_{X|\\Theta}$, depending on whether $X$ is discrete or continuous"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once a value $x$ of $X$ has been observed, _a complete answer to the Bayesian inference problem is provided by the posterior distribution $p_{\\Theta|X}(\\theta|x)$ or $f_{\\Theta|X}(\\theta|x)$ of $\\Theta$_. It is determined by Bayes' rule, and tells everything there is to know about $\\Theta$, given the available information. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](figures/fig_8_1.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Summary of Bayesian Inference__\n",
    "- We start with prior distribution $p_\\Theta$ or $f_\\Theta$ for the unknown random variable $\\Theta$\n",
    "- We have a model $p_{X|\\Theta}$ or $f_{X|\\Theta}$ of the observation vector $X$.\n",
    "- After observing $x$ of $X$, we form the posterior distribution of $\\Theta$, using Bayes' rule"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Bayes' Rule versions__\n",
    "\n",
    "$\\Theta$ discrete, $X$ discrete: $p_{\\Theta|X}(\\theta|x)$ = $\\frac{p_\\Theta(\\theta)p_{X|\\Theta}(x|\\theta)}{\\sum_{\\theta '} p_\\Theta(\\theta')p_{X|\\Theta}(x|\\theta')}$\n",
    "\n",
    "$\\Theta$ discrete, $X$ continuous: $p_{\\Theta|X}(\\theta|x)$ = $\\frac{p_\\Theta(\\theta)f_{X|\\Theta}(x|\\theta)}{\\sum_{\\theta '} p_\\Theta(\\theta')f_{X|\\Theta}(x|\\theta')}$\n",
    "\n",
    "$\\Theta$ continuous, $X$ discrete: $f_{\\Theta|X}(\\theta|x)$ = $\\frac{f_\\Theta(\\theta)p_{X|\\Theta}(x|\\theta)}{\\int f_\\Theta(\\theta')p_{X|\\Theta}(x|\\theta')d\\theta'}$\n",
    "\n",
    "$\\Theta$ continuous, $X$ continuous: $f_{\\Theta|X}(\\theta|x)$ = $\\frac{f_\\Theta(\\theta)f_{X|\\Theta}(x|\\theta)}{\\int f_\\Theta(\\theta')f_{X|\\Theta}(x|\\theta')d\\theta'}$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Another Summary__\n",
    "\n",
    "We have unknown $\\theta$, which is a value of the random variable $\\Theta$.  We assume the prior distribution of $\\Theta$, $p_\\Theta$ to be something reasonable, like a uniform distribution (this is the \"out and in the open for debate\" referenced in the introduction).  Then, we set up a conditional PDF, $p_{X|\\Theta}$, that represents the model of our observations, \"if our prior is true, I'm going to see $X$ like this. Now I can run an experiment, and take my observation vector, and use Bayes' rule to find a posterior PDF that I can use to predict $\\theta$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
